import statistics
import time
from benchmarks import *
from verification_algorithms import antichain_optimization_algorithm, counterexample_based_algorithm

try:
    os.mkdir('random_automata')
except FileExistsError:
    pass

try:
    os.mkdir('benchmarks_results')
except FileExistsError:
    pass


def run_benchmark(benchmark_type, save_file, parameters, nbr_points, range_start, range_end, range_step):
    """
    Runs several kinds of benchmarks. Given a set of fixed parameters in parameters, several values of some other
    variable are considered from range_start to range_end with a step of range_step. Given the parameters and a value
    for the variable in the range, the running time for both algorithms is evaluated nbr_points times. Consistency of
    the results from the algorithms is compared to the expected result given the benchmark being considered. The result
    of the benchmarks are printed as they are obtained and saved to the file with path save_file. We consider three
    types of benchmarks:

    - Random benchmarks:  when benchmark_type == "random", we evaluate the running time of the algorithms on random
    automata when increasing the number of objectives for Player 1 (with each function having 4 priorities). This number
    of objectives is considered in range(range_start, range_end, range_step). Given a number of objectives, both
    algorithms are evaluated on nbr_points random automata which are generated by the random_automaton function in
    benchmarks. These automata are generated (or retrieved from the random_automata folder if they already exist with
    the expected parameters) using the parameters in parameters. This parameters variables is expected to follow this
    format: [nbr_vertices, density, proba_even_general, proba_even_0, positivity], thus following the
    signature of the random_automaton function (without the number of objectives). Notice that we do not specify a value
    for name in that function, this is done by this run_benchmark function which appends which of the nbr_points
    automaton the current automaton corresponds to.

    - Intersection benchmarks for vertices:  when benchmark_type == "intersection_vertices", we evaluate the running
    time of the algorithms on the intersection example when considering the number of copies of the automaton (see
    documentation for intersection_example in benchmarks) to be in range(range_start, range_end, range_step). The
    parameters variable is expected to follow the format: [positivity] (True if the instance should be positive and
    False if it should be negative).

    - Intersection benchmarks for objectives:  when benchmark_type == "intersection_objectives", we evaluate the running
    time of the algorithms on the intersection example when considering the number of copies of the automaton (see
    documentation for intersection_example_objective_increase in benchmarks) to be in range(range_start, range_end,
    range_step), thus also increasing the number of objectives for Player 1. The parameters variable is expected to
    follow the format: [positivity] (True if the instance should be positive and False if it should be negative).

    The results of the benchmarks are saved in text format to save_file and are meant to be readable and parsable. They
    highlight the value of several statistics on the benchmarks as well as running times for each of the nbr_points
    times they were evaluated, for each set of parameter, and the mean value of those statistics and running times are
    also provided.

    :param benchmark_type: should be in ["random", "intersection_vertices", "intersection_objectives"]
    :param save_file: path to file containing benchmarks results.
    :param parameters: parameters to generate the automata in the benchmarks (see above for expected format).
    :param nbr_points: number of times the algorithms are evaluated given a set of parameters.
    :param range_start: start of the range for the variable being evaluated.
    :param range_end: end of the range for the variable being evaluated.
    :param range_step: step of the range for the variable being evaluated.
    """
    for i in list(range(range_start, range_end, range_step)):

        # lists to hold data for each of the nbr_points automata
        temp_antichain_sizes = []
        temp_nbr_payoffs_losing_player_0 = []
        temp_nbr_payoffs_realizable = []

        temp_counterexample_antichain_sizes = []
        temp_counterexample_nbr_calls = []
        temp_counterexample_nbr_calls_exists = []
        temp_counterexample_mean_time_calls_exists = []
        temp_counterexample_nbr_calls_dominated = []
        temp_counterexample_mean_time_calls_dominated = []

        temp_antichain_optimization_antichain_sizes = []
        temp_antichain_optimization_nbr_calls = []
        temp_antichain_optimization_nbr_calls1 = []
        temp_antichain_optimization_mean_time_calls1 = []
        temp_antichain_optimization_nbr_calls2 = []
        temp_antichain_optimization_mean_time_calls2 = []

        counterexample_times_process = []
        antichain_optimization_times_process = []

        counterexample_times_perf_counter = []
        antichain_optimization_times_perf_counter = []

        counterexample_times = []
        antichain_optimization_times = []

        for j in range(1, nbr_points + 1):

            if benchmark_type == "random":
                print("-------------------- trying to generate the " + str(j) + "th automaton with " + str(i)
                      + "functions --------------------")

                stats, aut, nbr, colors = random_automaton(parameters[0], parameters[1], i,  parameters[2],
                                                           parameters[3], parameters[4], str(j))
                gen_positivity = parameters[4]

            elif benchmark_type == "intersection_vertices":
                stats, aut, nbr, colors = intersection_example(i, negative_instance=not parameters[0])
                gen_positivity = parameters[0]

            elif benchmark_type == "intersection_objectives":
                stats, aut, nbr, colors = intersection_example_objective_increase(i,
                                                                                  negative_instance=not parameters[0])
                gen_positivity = parameters[0]

            else:
                class BenchmarkNotSupportedError(Exception):
                    pass

                raise BenchmarkNotSupportedError("This benchmark type is not supported.")

            temp_antichain_sizes.append(stats[0])
            temp_nbr_payoffs_losing_player_0.append(stats[1])
            temp_nbr_payoffs_realizable.append(stats[2])

            temp_counterexample_antichain_sizes.append(stats[3])
            temp_counterexample_nbr_calls_exists.append(stats[4][0])
            temp_counterexample_mean_time_calls_exists.append(float("%.2f" % statistics.mean(stats[4][1])))
            temp_counterexample_nbr_calls_dominated.append(stats[5][0])
            temp_counterexample_mean_time_calls_dominated.append(float("%.2f" % statistics.mean(stats[5][1])))
            temp_counterexample_nbr_calls.append(stats[4][0] + stats[5][0])

            temp_antichain_optimization_antichain_sizes.append(stats[6])
            temp_antichain_optimization_nbr_calls1.append(stats[7][0])
            temp_antichain_optimization_mean_time_calls1.append(float("%.2f" % statistics.mean(stats[7][1])))
            temp_antichain_optimization_nbr_calls2.append(stats[8][0])
            temp_antichain_optimization_mean_time_calls2.append(float("%.2f" % statistics.mean(stats[8][1])))
            temp_antichain_optimization_nbr_calls.append(stats[7][0] + stats[8][0])

            print(" ----- computing counterexample algorithm time -----")

            start_time_process = time.process_time()
            start_time_perf = time.perf_counter()
            start = time.time()

            positivity = counterexample_based_algorithm(aut, nbr, colors)

            end_time_process = time.process_time()
            end_time_perf = time.perf_counter()
            end = time.time()

            assert positivity == gen_positivity

            counterexample_times_process.append(float('%.2f' % (end_time_process - start_time_process)))
            counterexample_times_perf_counter.append(float('%.2f' % (end_time_perf - start_time_perf)))
            counterexample_times.append(float('%.2f' % (end - start)))

            print(" ----- computing antichain optimization time -----")

            start_time_process = time.process_time()
            start_time_perf = time.perf_counter()
            start = time.time()

            positivity = antichain_optimization_algorithm(aut, nbr, colors, is_payoff_realizable)

            end_time_process = time.process_time()
            end_time_perf = time.perf_counter()
            end = time.time()

            assert positivity == gen_positivity

            antichain_optimization_times_process.append(float('%.2f' % (end_time_process - start_time_process)))
            antichain_optimization_times_perf_counter.append(float('%.2f' % (end_time_perf - start_time_perf)))
            antichain_optimization_times.append(float('%.2f' % (end - start)))

            print(temp_antichain_sizes)
            print(temp_nbr_payoffs_losing_player_0)
            print(temp_nbr_payoffs_realizable)

            print(temp_counterexample_antichain_sizes)
            print(temp_counterexample_nbr_calls_exists)
            print(temp_counterexample_mean_time_calls_exists)
            print(temp_counterexample_nbr_calls_dominated)
            print(temp_counterexample_mean_time_calls_dominated)
            print(temp_counterexample_nbr_calls)

            print(counterexample_times_process)
            print(counterexample_times_perf_counter)

            print(temp_antichain_optimization_antichain_sizes)
            print(temp_antichain_optimization_nbr_calls1)
            print(temp_antichain_optimization_mean_time_calls1)
            print(temp_antichain_optimization_nbr_calls2)
            print(temp_antichain_optimization_mean_time_calls2)
            print(temp_antichain_optimization_nbr_calls)

            print(antichain_optimization_times_process)
            print(antichain_optimization_times_perf_counter)

        f = open(save_file, "a")

        f.write("Variable value used " + str(i) + "\n")
        f.write("Parameters " + str(parameters) + "\n")
        f.write("Number of objectives " + str(nbr) + "\n")

        f.write("Antichain sizes " + str(temp_antichain_sizes) + "\n")
        f.write("Number of payoffs losing P0 " + str(temp_nbr_payoffs_losing_player_0) + "\n")
        f.write("Number of payoffs realizable " + str(temp_nbr_payoffs_realizable) + "\n")

        f.write("CE antichain sizes " + str(temp_counterexample_antichain_sizes) + "\n")
        f.write("CE exists calls " + str(temp_counterexample_nbr_calls_exists) + "\n")
        f.write("CE exists calls times " + str(temp_counterexample_mean_time_calls_exists) + "\n")
        f.write("CE dominated calls " + str(temp_counterexample_nbr_calls_dominated) + "\n")
        f.write("CE dominated calls times " + str(temp_counterexample_mean_time_calls_dominated) + "\n")
        f.write("CE total nbr calls " + str(temp_counterexample_nbr_calls) + "\n")

        f.write("CE times process " + str(counterexample_times_process) + "\n")
        f.write("CE times perf " + str(counterexample_times_perf_counter) + "\n")
        f.write("CE times " + str(counterexample_times) + "\n")

        f.write("AO antichain sizes " + str(temp_antichain_optimization_antichain_sizes) + "\n")
        f.write("AO call1 calls " + str(temp_antichain_optimization_nbr_calls1) + "\n")
        f.write("AO call1 calls times " + str(temp_antichain_optimization_mean_time_calls1) + "\n")
        f.write("AO call2 calls " + str(temp_antichain_optimization_nbr_calls2) + "\n")
        f.write("AO call2 calls times " + str(temp_antichain_optimization_mean_time_calls2) + "\n")
        f.write("AO total nbr calls " + str(temp_antichain_optimization_nbr_calls) + "\n")

        f.write("AO times process " + str(antichain_optimization_times_process) + "\n")
        f.write("AO times perf " + str(antichain_optimization_times_perf_counter) + "\n")
        f.write("AO times " + str(antichain_optimization_times) + "\n")

        f.write("\n")

        f.write("%.2f" % statistics.mean(counterexample_times_process) + ", ")
        f.write("%.2f" % statistics.mean(counterexample_times_perf_counter) + ", ")
        f.write("%.2f" % statistics.mean(counterexample_times) + "\n")

        f.write("%.2f" % statistics.mean(antichain_optimization_times_process) + ", ")
        f.write("%.2f" % statistics.mean(antichain_optimization_times_perf_counter) + ", ")
        f.write("%.2f" % statistics.mean(antichain_optimization_times) + "\n")

        f.write("Mean antichain size " + "%.2f" % statistics.mean(temp_antichain_sizes) + "\n")
        f.write("Mean number losing payoffs " + "%.2f" % statistics.mean(temp_nbr_payoffs_losing_player_0) + "\n")
        f.write("Mean number realizable payoffs " + "%.2f" % statistics.mean(temp_nbr_payoffs_realizable) + "\n")

        f.write("CE mean antichain size " + "%.2f" % statistics.mean(temp_counterexample_antichain_sizes) + "\n")
        f.write("CE mean nbr exists calls " + "%.2f" % statistics.mean(temp_counterexample_nbr_calls_exists) + "\n")
        f.write("CE mean exists time " + "%.2f" % statistics.mean(temp_counterexample_mean_time_calls_exists) + "\n")
        f.write("CE mean nbr dominated calls " + "%.2f" % statistics.mean(temp_counterexample_nbr_calls_dominated) +
                "\n")
        f.write("CE mean dominated time " + "%.2f" % statistics.mean(temp_counterexample_mean_time_calls_dominated) +
                "\n")
        f.write("CE mean total nbr calls " + "%.2f" % statistics.mean(temp_counterexample_nbr_calls) + "\n")

        f.write("AO mean antichain size " + "%.2f" % statistics.mean(temp_antichain_optimization_antichain_sizes) +
                "\n")
        f.write("AO mean nbr call1 calls " + "%.2f" % statistics.mean(temp_antichain_optimization_nbr_calls1) + "\n")
        f.write("AO mean call1 time " + "%.2f" % statistics.mean(temp_antichain_optimization_mean_time_calls1) + "\n")
        f.write("AO mean nbr call2 calls " + "%.2f" % statistics.mean(temp_antichain_optimization_nbr_calls2) + "\n")
        f.write("AO mean call2 time " + "%.2f" % statistics.mean(temp_antichain_optimization_mean_time_calls2) + "\n")
        f.write("AO mean total nbr calls " + "%.2f" % statistics.mean(temp_antichain_optimization_nbr_calls) + "\n")

        f.write("\n")
        f.write("\n")
        f.write("\n")

        f.close()


run_benchmark("intersection_vertices",
              "benchmarks_results/intersection-vertices-positive.txt",
              [True],
              50,
              10000, 0, -500)

run_benchmark("intersection_vertices",
              "benchmarks_results/intersection-vertices-negative.txt",
              [False],
              50,
              10000, 0, -500)

run_benchmark("intersection_objectives",
              "benchmarks_results/intersection-objectives-positive.txt",
              [True],
              50,
              15, 0, -1)

run_benchmark("intersection_objectives",
              "benchmarks_results/intersection-objectives-negative.txt",
              [False],
              50,
              15, 0, -1)

run_benchmark("intersection_objectives",
              "benchmarks_results/intersection-objectives-positive.txt",
              [True],
              50,
              15, 0, -1)

run_benchmark("intersection_objectives",
              "benchmarks_results/intersection-objectives-negative.txt",
              [False],
              50,
              15, 0, -1)

run_benchmark("random",
              "benchmarks_results/random-positive.txt",
              [500, 0.2, 0.2, 0.1, True],
              50,
              15, 5, -1)

run_benchmark("random",
              "benchmarks_results/random-negative.txt",
              [500, 0.2, 0.2, 0.3, False],
              50,
              15, 6, -1)
